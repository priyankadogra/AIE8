{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Social Media ML Paper Agent - Multi-Agent LangGraph System\n",
        "\n",
        "This notebook implements a multi-agent system that creates social media posts about Machine Learning papers and verifies their correctness and platform appropriateness.\n",
        "\n",
        "## System Architecture\n",
        "\n",
        "The system consists of three main teams:\n",
        "\n",
        "1. **Content Creation Team**: Analyzes ML papers and creates social media content\n",
        "   - Paper Analyzer Agent\n",
        "   - Content Creator Agent\n",
        "   - Platform Optimizer Agent\n",
        "\n",
        "2. **Verification Team**: Verifies correctness and platform appropriateness\n",
        "   - Technical Reviewer Agent\n",
        "   - Style Checker Agent\n",
        "   - Fact Checker Agent\n",
        "\n",
        "3. **Meta-Supervisor**: Coordinates between teams and manages the overall workflow\n",
        "\n",
        "![System Architecture](https://i.imgur.com/placeholder.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Set up API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import functools\n",
        "import operator\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, List, Optional, TypedDict, Union, Dict, Annotated\n",
        "\n",
        "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain_core.tools import BaseTool, tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "from langgraph.graph import END, StateGraph, START\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def agent_node(state, agent, name):\n",
        "    \"\"\"Helper function to create agent nodes\"\"\"\n",
        "    result = agent.invoke(state)\n",
        "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n",
        "\n",
        "def create_agent(\n",
        "    llm: ChatOpenAI,\n",
        "    tools: list,\n",
        "    system_prompt: str,\n",
        ") -> str:\n",
        "    \"\"\"Create a function-calling agent and add it to the graph.\"\"\"\n",
        "    system_prompt += (\"\\nWork autonomously according to your specialty, using the tools available to you.\"\n",
        "    \" Do not ask for clarification.\"\n",
        "    \" Your other team members (and other teams) will collaborate with you with their own specialties.\"\n",
        "    \" You are chosen for a reason!\")\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                system_prompt,\n",
        "            ),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "        ]\n",
        "    )\n",
        "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "    executor = AgentExecutor(agent=agent, tools=tools)\n",
        "    return executor\n",
        "\n",
        "def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:\n",
        "    \"\"\"An LLM-based router.\"\"\"\n",
        "    options = [\"FINISH\"] + members\n",
        "    function_def = {\n",
        "        \"name\": \"route\",\n",
        "        \"description\": \"Select the next role.\",\n",
        "        \"parameters\": {\n",
        "            \"title\": \"routeSchema\",\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"next\": {\n",
        "                    \"title\": \"Next\",\n",
        "                    \"anyOf\": [\n",
        "                        {\"enum\": options},\n",
        "                    ],\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"next\"],\n",
        "        },\n",
        "    }\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            (\n",
        "                \"system\",\n",
        "                \"Given the conversation above, who should act next?\"\n",
        "                \" Or should we FINISH? Select one of: {options}\",\n",
        "            ),\n",
        "        ]\n",
        "    ).partial(options=str(options), team_members=\", \".join(members))\n",
        "    return (\n",
        "        prompt\n",
        "        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
        "        | JsonOutputFunctionsParser()\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create working directory for storing content\n",
        "os.makedirs('./social_media_content', exist_ok=True)\n",
        "\n",
        "def create_random_subdirectory():\n",
        "    random_id = str(uuid.uuid4())[:8]\n",
        "    subdirectory_path = os.path.join('./social_media_content', random_id)\n",
        "    os.makedirs(subdirectory_path, exist_ok=True)\n",
        "    return subdirectory_path\n",
        "\n",
        "WORKING_DIRECTORY = Path(create_random_subdirectory())\n",
        "\n",
        "# Initialize search tool\n",
        "tavily_tool = TavilySearchResults(max_results=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def search_ml_paper_info(\n",
        "    query: Annotated[str, \"Search query for ML paper information\"]\n",
        ") -> str:\n",
        "    \"\"\"Search for information about machine learning papers, authors, or concepts.\"\"\"\n",
        "    return tavily_tool.invoke(query)\n",
        "\n",
        "@tool\n",
        "def create_social_media_post(\n",
        "    platform: Annotated[str, \"Social media platform (twitter, linkedin, instagram)\"],\n",
        "    content: Annotated[str, \"Main content of the post\"],\n",
        "    hashtags: Annotated[List[str], \"List of relevant hashtags\"],\n",
        "    file_name: Annotated[str, \"File name to save the post\"]\n",
        ") -> str:\n",
        "    \"\"\"Create a social media post optimized for a specific platform.\"\"\"\n",
        "    \n",
        "    platform_configs = {\n",
        "        \"twitter\": {\"char_limit\": 280, \"hashtag_limit\": 5},\n",
        "        \"linkedin\": {\"char_limit\": 3000, \"hashtag_limit\": 10},\n",
        "        \"instagram\": {\"char_limit\": 2200, \"hashtag_limit\": 30}\n",
        "    }\n",
        "    \n",
        "    config = platform_configs.get(platform.lower(), platform_configs[\"twitter\"])\n",
        "    \n",
        "    # Format hashtags\n",
        "    formatted_hashtags = [f\"#{tag.replace('#', '')}\" for tag in hashtags[:config[\"hashtag_limit\"]]]\n",
        "    hashtag_string = \" \".join(formatted_hashtags)\n",
        "    \n",
        "    # Create platform-specific post\n",
        "    if platform.lower() == \"twitter\":\n",
        "        post = f\"{content}\\n\\n{hashtag_string}\"\n",
        "    elif platform.lower() == \"linkedin\":\n",
        "        post = f\"{content}\\n\\n{hashtag_string}\\n\\n#MachineLearning #AI #Research\"\n",
        "    else:  # Instagram\n",
        "        post = f\"{content}\\n\\n{hashtag_string}\"\n",
        "    \n",
        "    # Truncate if necessary\n",
        "    if len(post) > config[\"char_limit\"]:\n",
        "        post = post[:config[\"char_limit\"]-3] + \"...\"\n",
        "    \n",
        "    # Save to file\n",
        "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
        "        file.write(f\"Platform: {platform}\\n\")\n",
        "        file.write(f\"Character count: {len(post)}\\n\")\n",
        "        file.write(f\"Hashtag count: {len(formatted_hashtags)}\\n\")\n",
        "        file.write(\"\\n--- POST CONTENT ---\\n\")\n",
        "        file.write(post)\n",
        "    \n",
        "    return f\"Social media post created and saved to {file_name}. Character count: {len(post)}\"\n",
        "\n",
        "@tool\n",
        "def read_content_file(\n",
        "    file_name: Annotated[str, \"Name of the file to read\"]\n",
        ") -> str:\n",
        "    \"\"\"Read the content of a saved file.\"\"\"\n",
        "    try:\n",
        "        with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
        "            return file.read()\n",
        "    except FileNotFoundError:\n",
        "        return f\"File {file_name} not found.\"\n",
        "\n",
        "@tool\n",
        "def verify_technical_accuracy(\n",
        "    paper_info: Annotated[str, \"Information about the ML paper\"],\n",
        "    social_post: Annotated[str, \"Social media post content\"]\n",
        ") -> str:\n",
        "    \"\"\"Verify that the social media post accurately represents the technical content of the paper.\"\"\"\n",
        "    \n",
        "    verification_report = f\"\"\"\n",
        "    TECHNICAL ACCURACY VERIFICATION REPORT\n",
        "    \n",
        "    Paper Information:\n",
        "    {paper_info}\n",
        "    \n",
        "    Social Media Post:\n",
        "    {social_post}\n",
        "    \n",
        "    Verification Checklist:\n",
        "    □ Technical claims are accurate\n",
        "    □ No oversimplification that leads to misinformation\n",
        "    □ Proper attribution to authors\n",
        "    □ Results and claims are not exaggerated\n",
        "    □ Methodology is correctly described\n",
        "    \n",
        "    Recommendations:\n",
        "    - Check for any technical inaccuracies\n",
        "    - Ensure claims are supported by the paper\n",
        "    - Verify that complex concepts are simplified appropriately\n",
        "    \"\"\"\n",
        "    \n",
        "    return verification_report\n",
        "\n",
        "@tool\n",
        "def check_platform_style(\n",
        "    platform: Annotated[str, \"Social media platform\"],\n",
        "    post_content: Annotated[str, \"Social media post content\"]\n",
        ") -> str:\n",
        "    \"\"\"Check if the post follows the style and best practices for the target platform.\"\"\"\n",
        "    \n",
        "    platform_guidelines = {\n",
        "        \"twitter\": {\n",
        "            \"tone\": \"Concise, engaging, conversational\",\n",
        "            \"best_practices\": [\"Use threads for complex topics\", \"Include relevant hashtags\", \"Tag authors if possible\"],\n",
        "            \"char_limit\": 280\n",
        "        },\n",
        "        \"linkedin\": {\n",
        "            \"tone\": \"Professional, informative, thought-provoking\",\n",
        "            \"best_practices\": [\"Start with a hook\", \"Use line breaks for readability\", \"End with a question or call-to-action\"],\n",
        "            \"char_limit\": 3000\n",
        "        },\n",
        "        \"instagram\": {\n",
        "            \"tone\": \"Visual, engaging, accessible\",\n",
        "            \"best_practices\": [\"Use emojis appropriately\", \"Include many relevant hashtags\", \"Make it visually appealing\"],\n",
        "            \"char_limit\": 2200\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    guidelines = platform_guidelines.get(platform.lower(), platform_guidelines[\"twitter\"])\n",
        "    \n",
        "    style_report = f\"\"\"\n",
        "    PLATFORM STYLE VERIFICATION REPORT\n",
        "    \n",
        "    Platform: {platform}\n",
        "    Expected Tone: {guidelines['tone']}\n",
        "    Character Limit: {guidelines['char_limit']}\n",
        "    \n",
        "    Post Content:\n",
        "    {post_content}\n",
        "    \n",
        "    Style Checklist:\n",
        "    □ Appropriate tone for platform\n",
        "    □ Within character limits\n",
        "    □ Follows platform best practices\n",
        "    □ Engaging and accessible language\n",
        "    □ Proper use of hashtags and mentions\n",
        "    \n",
        "    Best Practices for {platform}:\n",
        "    {chr(10).join(['- ' + practice for practice in guidelines['best_practices']])}\n",
        "    \n",
        "    Recommendations:\n",
        "    - Ensure the tone matches the platform audience\n",
        "    - Check character count and hashtag usage\n",
        "    - Verify engagement potential\n",
        "    \"\"\"\n",
        "    \n",
        "    return style_report\n",
        "\n",
        "@tool\n",
        "def save_verification_report(\n",
        "    report_content: Annotated[str, \"Verification report content\"],\n",
        "    file_name: Annotated[str, \"File name for the report\"]\n",
        ") -> str:\n",
        "    \"\"\"Save a verification report to a file.\"\"\"\n",
        "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
        "        file.write(report_content)\n",
        "    return f\"Verification report saved to {file_name}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Team States and Complete System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Team State Definitions\n",
        "class ContentTeamState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    team_members: List[str]\n",
        "    next: str\n",
        "\n",
        "class VerificationTeamState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    team_members: List[str]\n",
        "    next: str\n",
        "\n",
        "class MetaState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    next: str\n",
        "\n",
        "# Helper function to track files\n",
        "def content_prelude(state):\n",
        "    written_files = []\n",
        "    if WORKING_DIRECTORY.exists():\n",
        "        try:\n",
        "            written_files = [f.relative_to(WORKING_DIRECTORY) for f in WORKING_DIRECTORY.rglob(\"*\") if f.is_file()]\n",
        "        except:\n",
        "            pass\n",
        "    return state\n",
        "\n",
        "# Initialize LLMs with different models for different purposes\n",
        "content_llm = ChatOpenAI(model=\"gpt-4o-mini\")  # Fast and cost-effective for content creation\n",
        "verification_llm = ChatOpenAI(model=\"gpt-4o\")  # More powerful model for critical verification tasks\n",
        "meta_llm = ChatOpenAI(model=\"gpt-4o-mini\")    # Efficient for coordination\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/dq/7_w0fl7j3s3fs7t6pbr2jtmh0000gq/T/ipykernel_72504/1351296439.py:63: LangChainDeprecationWarning: The method `BaseChatOpenAI.bind_functions` was deprecated in langchain-openai 0.2.1 and will be removed in 1.0.0. Use :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind_tools` instead.\n",
            "  | llm.bind_functions(functions=[function_def], function_call=\"route\")\n"
          ]
        }
      ],
      "source": [
        "# Create Content Creation Team Agents\n",
        "paper_analyzer_agent = create_agent(\n",
        "    content_llm,\n",
        "    [search_ml_paper_info],\n",
        "    \"You are an expert ML researcher who analyzes papers to extract key insights for social media content.\"\n",
        ")\n",
        "paper_analyzer_node = functools.partial(agent_node, agent=paper_analyzer_agent, name=\"PaperAnalyzer\")\n",
        "\n",
        "content_creator_agent = create_agent(\n",
        "    content_llm,\n",
        "    [create_social_media_post, read_content_file],\n",
        "    \"You are a social media content creator who makes complex ML research accessible and engaging.\"\n",
        ")\n",
        "content_creator_node = functools.partial(agent_node, agent=content_creator_agent, name=\"ContentCreator\")\n",
        "\n",
        "platform_optimizer_agent = create_agent(\n",
        "    content_llm,\n",
        "    [create_social_media_post, read_content_file, check_platform_style],\n",
        "    \"You optimize content for specific social media platforms considering character limits and best practices.\"\n",
        ")\n",
        "platform_optimizer_node = functools.partial(agent_node, agent=platform_optimizer_agent, name=\"PlatformOptimizer\")\n",
        "\n",
        "# Content Team Supervisor\n",
        "content_supervisor_agent = create_team_supervisor(\n",
        "    content_llm,\n",
        "    \"You manage a content creation team. Coordinate: analyze paper → create content → optimize for platform. When ready, respond with FINISH.\",\n",
        "    [\"PaperAnalyzer\", \"ContentCreator\", \"PlatformOptimizer\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Verification Team Agents\n",
        "technical_reviewer_agent = create_agent(\n",
        "    verification_llm,\n",
        "    [read_content_file, verify_technical_accuracy, search_ml_paper_info, save_verification_report],\n",
        "    \"You verify technical accuracy of ML content, ensuring no misinformation or oversimplification.\"\n",
        ")\n",
        "technical_reviewer_node = functools.partial(agent_node, agent=technical_reviewer_agent, name=\"TechnicalReviewer\")\n",
        "\n",
        "style_checker_agent = create_agent(\n",
        "    verification_llm,\n",
        "    [read_content_file, check_platform_style, save_verification_report],\n",
        "    \"You ensure content fits platform requirements including tone, character limits, and engagement strategies.\"\n",
        ")\n",
        "style_checker_node = functools.partial(agent_node, agent=style_checker_agent, name=\"StyleChecker\")\n",
        "\n",
        "fact_checker_agent = create_agent(\n",
        "    verification_llm,\n",
        "    [read_content_file, search_ml_paper_info, save_verification_report],\n",
        "    \"You fact-check claims in social media content and cross-reference with reliable sources.\"\n",
        ")\n",
        "fact_checker_node = functools.partial(agent_node, agent=fact_checker_agent, name=\"FactChecker\")\n",
        "\n",
        "# Verification Team Supervisor\n",
        "verification_supervisor_agent = create_team_supervisor(\n",
        "    verification_llm,\n",
        "    \"You manage a verification team. Ensure comprehensive verification: technical accuracy → platform style → fact-checking. When complete, respond with FINISH.\",\n",
        "    [\"TechnicalReviewer\", \"StyleChecker\", \"FactChecker\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build Content Creation Graph\n",
        "content_graph = StateGraph(ContentTeamState)\n",
        "content_graph.add_node(\"PaperAnalyzer\", paper_analyzer_node)\n",
        "content_graph.add_node(\"ContentCreator\", content_creator_node)\n",
        "content_graph.add_node(\"PlatformOptimizer\", platform_optimizer_node)\n",
        "content_graph.add_node(\"ContentSupervisor\", content_supervisor_agent)\n",
        "\n",
        "content_graph.add_edge(\"PaperAnalyzer\", \"ContentSupervisor\")\n",
        "content_graph.add_edge(\"ContentCreator\", \"ContentSupervisor\")\n",
        "content_graph.add_edge(\"PlatformOptimizer\", \"ContentSupervisor\")\n",
        "\n",
        "content_graph.add_conditional_edges(\n",
        "    \"ContentSupervisor\",\n",
        "    lambda x: x[\"next\"],\n",
        "    {\n",
        "        \"PaperAnalyzer\": \"PaperAnalyzer\",\n",
        "        \"ContentCreator\": \"ContentCreator\",\n",
        "        \"PlatformOptimizer\": \"PlatformOptimizer\",\n",
        "        \"FINISH\": END,\n",
        "    },\n",
        ")\n",
        "\n",
        "content_graph.set_entry_point(\"ContentSupervisor\")\n",
        "compiled_content_graph = content_graph.compile()\n",
        "compiled_content_graph\n",
        "\n",
        "# Build Verification Graph\n",
        "verification_graph = StateGraph(VerificationTeamState)\n",
        "verification_graph.add_node(\"TechnicalReviewer\", technical_reviewer_node)\n",
        "verification_graph.add_node(\"StyleChecker\", style_checker_node)\n",
        "verification_graph.add_node(\"FactChecker\", fact_checker_node)\n",
        "verification_graph.add_node(\"VerificationSupervisor\", verification_supervisor_agent)\n",
        "\n",
        "verification_graph.add_edge(\"TechnicalReviewer\", \"VerificationSupervisor\")\n",
        "verification_graph.add_edge(\"StyleChecker\", \"VerificationSupervisor\")\n",
        "verification_graph.add_edge(\"FactChecker\", \"VerificationSupervisor\")\n",
        "\n",
        "verification_graph.add_conditional_edges(\n",
        "    \"VerificationSupervisor\",\n",
        "    lambda x: x[\"next\"],\n",
        "    {\n",
        "        \"TechnicalReviewer\": \"TechnicalReviewer\",\n",
        "        \"StyleChecker\": \"StyleChecker\",\n",
        "        \"FactChecker\": \"FactChecker\",\n",
        "        \"FINISH\": END,\n",
        "    },\n",
        ")\n",
        "\n",
        "verification_graph.set_entry_point(\"VerificationSupervisor\")\n",
        "compiled_verification_graph = verification_graph.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ System successfully compiled!\n",
            "📊 Graphs created:\n",
            "   - Content Creation Team\n",
            "   - Verification Team\n",
            "   - Meta-Supervisor\n"
          ]
        }
      ],
      "source": [
        "# Helper functions for meta-supervisor\n",
        "def enter_content_chain(message: str, members: List[str]):\n",
        "    return {\"messages\": [HumanMessage(content=message)], \"team_members\": members}\n",
        "\n",
        "def enter_verification_chain(message: str, members: List[str]):\n",
        "    return {\"messages\": [HumanMessage(content=message)], \"team_members\": members}\n",
        "\n",
        "# Create chain interfaces\n",
        "content_chain = (\n",
        "    functools.partial(enter_content_chain, members=list(content_graph.nodes))\n",
        "    | compiled_content_graph\n",
        ")\n",
        "\n",
        "verification_chain = (\n",
        "    functools.partial(enter_verification_chain, members=list(verification_graph.nodes))\n",
        "    | compiled_verification_graph\n",
        ")\n",
        "\n",
        "# Helper functions for meta state management\n",
        "def get_last_message(state: MetaState) -> str:\n",
        "    return state[\"messages\"][-1].content\n",
        "\n",
        "def join_graph(response: dict):\n",
        "    return {\"messages\": [response[\"messages\"][-1]]}\n",
        "\n",
        "# Meta-supervisor\n",
        "meta_supervisor_agent = create_team_supervisor(\n",
        "    meta_llm,\n",
        "    \"You coordinate between content creation and verification teams. First create content, then verify it. When both are complete, respond with FINISH.\",\n",
        "    [\"ContentTeam\", \"VerificationTeam\"],\n",
        ")\n",
        "\n",
        "# Build Meta-Supervisor Graph\n",
        "meta_graph = StateGraph(MetaState)\n",
        "meta_graph.add_node(\"ContentTeam\", get_last_message | content_chain | join_graph)\n",
        "meta_graph.add_node(\"VerificationTeam\", get_last_message | verification_chain | join_graph)\n",
        "meta_graph.add_node(\"MetaSupervisor\", meta_supervisor_agent)\n",
        "\n",
        "meta_graph.add_edge(\"ContentTeam\", \"MetaSupervisor\")\n",
        "meta_graph.add_edge(\"VerificationTeam\", \"MetaSupervisor\")\n",
        "meta_graph.add_conditional_edges(\n",
        "    \"MetaSupervisor\",\n",
        "    lambda x: x[\"next\"],\n",
        "    {\n",
        "        \"ContentTeam\": \"ContentTeam\",\n",
        "        \"VerificationTeam\": \"VerificationTeam\",\n",
        "        \"FINISH\": END,\n",
        "    },\n",
        ")\n",
        "\n",
        "meta_graph.set_entry_point(\"MetaSupervisor\")\n",
        "compiled_meta_graph = meta_graph.compile()\n",
        "\n",
        "print(\"✅ System successfully compiled!\")\n",
        "print(\"📊 Graphs created:\")\n",
        "print(\"   - Content Creation Team\")\n",
        "print(\"   - Verification Team\") \n",
        "print(\"   - Meta-Supervisor\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Usage - Create a Twitter Post about Transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting social media post creation and verification process...\n",
            "📁 Working directory: social_media_content/b71e6145\n",
            "============================================================\n",
            "{'MetaSupervisor': {'next': 'ContentTeam'}}\n",
            "---\n",
            "{'ContentTeam': {'messages': [HumanMessage(content='The LinkedIn post about the paper \"Attention Is All You Need\" by Vaswani et al. has been successfully optimized and saved. It highlights key contributions and engages the audience effectively. You can find the post in the file named **transformer_paper_linkedin_post.txt**.', additional_kwargs={}, response_metadata={}, name='PlatformOptimizer')]}}\n",
            "---\n",
            "{'MetaSupervisor': {'next': 'VerificationTeam'}}\n",
            "---\n",
            "{'VerificationTeam': {'messages': [HumanMessage(content='The verification report for the LinkedIn post about the paper \"Attention Is All You Need\" has been successfully created and saved. It confirms that the post accurately summarizes the paper\\'s contributions, maintains a professional tone, adheres to character limits, and uses appropriate hashtags for engagement. \\n\\nIf you need any further assistance or have another task, feel free to let me know!', additional_kwargs={}, response_metadata={}, name='FactChecker')]}}\n",
            "---\n",
            "{'MetaSupervisor': {'next': 'FINISH'}}\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Create a new working directory for this example\n",
        "WORKING_DIRECTORY = Path(create_random_subdirectory())\n",
        "\n",
        "# Example: Create a Linkedin post about the famous Transformer paper\n",
        "example_request = \"\"\"\n",
        "Create a LinkedIn post about the paper \"Attention Is All You Need\" by Vaswani et al. \n",
        "This is the foundational Transformer paper that introduced the self-attention mechanism \n",
        "and revolutionized natural language processing.\n",
        "\n",
        "Key points:\n",
        "- Introduced the Transformer architecture\n",
        "- Self-attention mechanism eliminates need for RNNs/CNNs\n",
        "- Parallel processing advantages\n",
        "- Foundation for BERT, GPT, and modern LLMs\n",
        "- Achieved state-of-the-art translation results\n",
        "\n",
        "Target platform: LinkedIn\n",
        "Make it engaging and accessible to a broad audience!\n",
        "\"\"\"\n",
        "\n",
        "print(\"🚀 Starting social media post creation and verification process...\")\n",
        "print(f\"📁 Working directory: {WORKING_DIRECTORY}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for step in compiled_meta_graph.stream(\n",
        "    {\"messages\": [HumanMessage(content=example_request)]},\n",
        "    {\"recursion_limit\": 50}\n",
        "):\n",
        "    if \"__end__\" not in step:\n",
        "        print(step)\n",
        "        print(\"---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View Generated Content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display all files created during the process\n",
        "print(\"📄 Files created during the process:\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "if WORKING_DIRECTORY.exists():\n",
        "    for file_path in WORKING_DIRECTORY.rglob(\"*\"):\n",
        "        if file_path.is_file():\n",
        "            print(f\"\\n📝 {file_path.name}:\")\n",
        "            print(\"-\" * 30)\n",
        "            with open(file_path, 'r') as f:\n",
        "                content = f.read()\n",
        "                print(content)\n",
        "else:\n",
        "    print(\"❌ No files found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
